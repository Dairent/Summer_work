import requests
from bs4 import BeautifulSoup

#Здесь я на основе первого файла - создал парсер в котором хотел использовать часть информации с основной страницы, которая уже подходит для выполнения задания -
# - обьединить с новой (хотел с помощью циклов и прохождение по каждой из ссылок - собрать текст с каждой новой "спотлайт" страницы информацию о содержащемся тексте и обьединить с...)
# ... с уже имеющейся информацией с начальной страницы (титульной картинке, ссылке и названию "спотлайта")

start_URL = "https://www.nature.com/wls/spotlights/"
queue = []
itog = []
texts = []

def Crawler():

    r = requests.get(start_URL)
    r.text
    soup_1 = BeautifulSoup(r.text, "html.parser")

    spotlights = soup_1.findAll('div', class_="section clearfix spotlightSection")

    for spotlight in spotlights:
        link = "https://www.nature.com" + spotlight.find('a', class_="plumColor px22 siteFontNormalC").get('href')
        name = spotlight.find('a', class_="plumColor px22 siteFontNormalC").text.strip()
        picture = spotlight.find('div', class_="wrappedImg padtop3px").find('img').get('src')
        itog.append([link, name, picture])

    for spotlight in spotlights:
        link = "https://www.nature.com" + spotlight.find('a', class_="plumColor px22 siteFontNormalC").get('href')
        queue.append(link)

    for i in queue:
        print(i)
        r = requests.get(i)
        r.text
        soup_2 = BeautifulSoup(r.text, "html.parser")

   #for i in range(len(spotlights)):
   #    itog[i] = itog[i] + texts[i]

    for i in range(len(spotlights)):
        print(itog[i])

    for i in range(len((texts))):
        print(texts[i])


if __name__ == '__main__':
    Crawler()
